# Data extraction from pdfs
This is a project in which you extract some urls from a page of books in PDF and then use xdotools to download the books (there are simpler methods to download the books, the objective of this exercise is to do it with **desktop automation**), then extract the text of the books and make a count of the most frequent words and export them in a word cloud.

## Requirements
linux kernel operating system

## Installation of the libraries
To use this code, make sure you have the following libraries installed in your Python environment. You can install them by executing the following commands:

pip install beautifulsoup4 <br>
pip install alive-progress <br>
pip install PyPDF2 <br>
pip install nltk <br>
pip install scikit-learn <br>
pip install pandas <br>
pip install circlify <br>
pip install seaborn <br>
pip install matplotlib <br>

In addition, you will need to download additional data for NLTK. You can do this by running the following Python code:

import nltk <br>
nltk.download('punkt')

## get_urls()
Obtain the URLs of the books from the repository website.
## get_pdfs()
Run the Desktop Automation Shell script using the book_url.txt generated by get_urls()
## to_text()
convert all pdf to text
## word_freq()
calculates the frequencies of the words and generates the word cloud

